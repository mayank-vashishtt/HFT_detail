
problem 
read -- understand -- ideate -- implement -- debug -- accepted 

what is correctness of your ideate part ?
efficency of your ideate part ? 


time complexity 
means number of elementary instruction it takes

best case complexity
average case complexity
worst case complexity

1.
time complexity for this 



int count = 0 ; 
for ( int i =0 ; i<N ; i++){
    for(int j=0 ; j<i; j++){
        count++:

    }
}


Big O Notation 

Big O Notation is a mathematical way to describe how the running time or space 
requirements of an algorithm grow as the input size increases.

Key Points:
It focuses on the term that grows the fastest as the input size (N) increases.
It ignores constants and lower-order terms because they become insignificant 
for large inputs.
It helps compare the efficiency of algorithms, especially for large datasets.
Example:
If an algorithm takes N² + 5N + 10 steps, Big O notation expresses this as O(N²), 
because N² dominates as N gets large.

Common Big O Classes:
O(1): Constant time (does not depend on input size)
O(log N): Logarithmic time
O(N): Linear time
O(N²): Quadratic time
O(2^N): Exponential time
Big O gives a high-level understanding of algorithm performance and scalability. 




Order Notation 
O(f(N)) --- 
theta(f(N))
omega(f(N))
Examples:

1. Big O Notation (O(f(N))) – Upper Bound Example:
    Suppose an algorithm takes at most 5N² + 3N + 10 steps for input size N.
    We say its time complexity is O(N²), because for large N, the running time will not exceed a constant times N².

2. Theta Notation (θ(f(N))) – Tight Bound Example:
    Suppose an algorithm always takes exactly 2N + 3 steps for input size N.
    We say its time complexity is θ(N), because it grows linearly and both the upper and lower bounds are proportional to N.

3. Omega Notation (Ω(f(N))) – Lower Bound Example:
    Suppose an algorithm takes at least N log N steps, but could take more depending on the input.
    We say its time complexity is Ω(N log N), because it will never be faster than a constant times N log N for large N.


1. Big O Notation (O(f(N)))

Describes the upper bound of an algorithm’s running time.
It tells you the worst-case scenario: the algorithm will not take 
more than O(f(N)) time for large N.
Example: If an algorithm is O(N²), it will not take more than 
c·N² steps for some constant c.

2. Theta Notation (θ(f(N)))

Describes the tight bound (both upper and lower) of an algorithm’s running time.
It means the algorithm always takes θ(f(N)) time for large N, both 
in the best and worst cases.
Example: If an algorithm is θ(N²), it always takes time proportional to N².

3. Omega Notation (Ω(f(N)))

Describes the lower bound of an algorithm’s running time.
It tells you the best-case scenario: the algorithm will take at 
least Ω(f(N)) time for large N.
Example: If an algorithm is Ω(N), it will take at least c·N steps 
for some constant c.
In summary:

O(f(N)) = at most f(N) (upper bound)
θ(f(N)) = exactly f(N) (tight bound)
Ω(f(N)) = at least f(N) (lower bound)


Note on the difference between Big O and Theta:

- Big O (O(f(N))) gives an upper bound. It tells you the algorithm will not take more than a certain amount of time (worst case), but it could be faster.
- Theta (θ(f(N))) gives a tight bound. It tells you the algorithm will always take about that much time (both upper and lower bounds), so it’s an exact growth rate.

In short:
- Big O = “at most”
- Theta = “exactly” (both “at least” and “at most”)


nesting means multiplying 
sequence do addition 


Example Problem
Suppose you have a problem where N can be up to 100,000 (N ≤ 10⁵).

Approach 1: Brute Force (O(N²))
You write two nested loops, each going up to N.
For N = 100,000, the number of operations is about 10¹⁰ (100,000 × 100,000).
Most computers can do about 10⁸ operations per second, so this would take 
about 100 seconds—too slow!
Conclusion: O(N²) is not acceptable for N = 10⁵. You should not code this approach.


Approach 2: Efficient (O(N log N))
You find a way to solve the problem using sorting or a data structure, 
which takes O(N log N) time.
For N = 100,000, N log N ≈ 100,000 × 17 ≈ 1,700,000 operations.
This is much less than 10⁸, so it will run in less than a second.
Conclusion: O(N log N) is acceptable for N = 10⁵. You can code this approach.


Read sheet.txt, lines 1 to 100

Let's go deep with an example to show how to check time complexity based on input size and then decide if you can code your solution:

Example Problem
Suppose you have a problem where N can be up to 100,000 (N ≤ 10⁵).

Approach 1: Brute Force (O(N²))
You write two nested loops, each going up to N.
For N = 100,000, the number of operations is about 10¹⁰ (100,000 × 100,000).
Most computers can do about 10⁸ operations per second, so this would take about 100 seconds—too slow!
Conclusion: O(N²) is not acceptable for N = 10⁵. You should not code this approach.
Approach 2: Efficient (O(N log N))
You find a way to solve the problem using sorting or a data structure, which takes O(N log N) time.
For N = 100,000, N log N ≈ 100,000 × 17 ≈ 1,700,000 operations.
This is much less than 10⁸, so it will run in less than a second.
Conclusion: O(N log N) is acceptable for N = 10⁵. You can code this approach.


Steps to Decide:
Estimate your algorithm’s time complexity.
Check the maximum input size (N) in the problem.
Calculate the number of operations for your approach.
Compare with what a computer can handle (about 10⁸ operations per second).
If it fits, start coding. If not, optimize your approach.



Amortization in algorithms is a technique to analyze the average time per
operation over a sequence of operations, even if some operations are expensive.


Memory complexity 
Memory complexity (also called space complexity) measures the amount of 
memory an algorithm or program needs to run, as a function of the input size.

Key Points:
- It includes memory for variables, data structures, function call stack, and 
any extra space used by the algorithm.
- Like time complexity, it is usually expressed in Big O notation 
(e.g., O(1), O(N), O(N²)).


Example 1: Constant Space (O(1))
```cpp
int sum = 0;
for (int i = 0; i < N; i++) {
    sum += arr[i];
}
```
Here, only a few variables are used, so memory complexity is O(1).

Example 2: Linear Space (O(N))
```cpp
int* copy = new int[N];
for (int i = 0; i < N; i++) {
    copy[i] = arr[i];
}
```
Here, an extra array of size N is used, so memory complexity is O(N).

Example 3: Quadratic Space (O(N²))
```cpp
int** matrix = new int*[N];
for (int i = 0; i < N; i++) {
    matrix[i] = new int[N];
}
```
Here, a 2D array of size N×N is used, so memory complexity is O(N²).

### Common Data Types and Their Sizes
| Data Type | Typical Size (bytes) | Size (bits) |
|-----------|----------------------|-------------|
| bool      | 1                    | 8           |
| char      | 1                    | 8           |
| int       | 4                    | 32          |
| float     | 4                    | 32          |
| double    | 8                    | 64          |
| long      | 8                    | 64          |
| pointer   | 8 (on 64-bit system) | 64          |

Note: Sizes can vary by language and system, but these are common values 
for C++/Java on modern 64-bit systems.*

Summary
- Memory complexity helps you understand how much space your 
solution will use as the input grows.
- Always consider both time and memory complexity 
when designing algorithms, especially for large inputs.



what is NP-HARD problems ?

NP-Hard problems are problems that are very hard to solve quickly 
(no fast algorithm is known).

Simple Explanation:
- If you can solve an NP-Hard problem fast, you can solve all problems in NP fast too.
- But, for NP-Hard problems, even checking if a solution is correct might not be easy.
- These problems usually take a huge amount of time to solve as the input size grows.

Example of an NP-Hard Problem:**
Traveling Salesman Problem (TSP):
- You are given a list of cities and the distances between each pair of cities.
- The task: Find the shortest possible route that visits every city exactly once and returns to the starting city.
- For a small number of cities, you can try all possible routes, but as the number of cities increases, the number of possible routes grows extremely fast (factorial growth).
- No one knows a fast way to solve this for large numbers of cities.

Other Examples:**
- Knapsack Problem
- Sudoku (generalized to large boards)
- Subset Sum Problem

Summary:
NP-Hard problems are some of the hardest problems in computer science. If you find a fast way to solve one, you can solve many other hard problems fast too!



Calcuating time complexity in recursion 

using master theorem

The Master Theorem is a formula to quickly find the time complexity of 
many recursive algorithms, especially those that divide a problem into 
smaller subproblems.


It applies to recurrences of the form:
    T(N) = a * T(N/b) + f(N)
where:
    - a = number of subproblems
    - N/b = size of each subproblem
    - f(N) = extra work done outside the recursive calls


Example: Fibonacci Series (Simple Recursion)
The classic recursive Fibonacci function:
```cpp
int fib(int n) {
    if (n <= 1) return n;
    return fib(n-1) + fib(n-2);
}
```
The recurrence for this is:
    T(N) = T(N-1) + T(N-2) + O(1)
This is NOT directly in the master theorem form, but we can estimate its growth:
- Each call makes 2 more calls (except at the base case), so the number of calls doubles at each level.
- The total number of calls is about 2^N.
- So, the time complexity is O(2^N) (exponential time).

### Master Theorem Example (Divide and Conquer)
For recurrences like:
    T(N) = 2 * T(N/2) + O(N)
This matches merge sort, where:
    - a = 2 (two subproblems)
    - b = 2 (each half size)
    - f(N) = O(N) (merging step)
By the master theorem, T(N) = O(N log N).

### Summary
- Use the master theorem for recurrences like T(N) = a*T(N/b) + f(N).
- For Fibonacci, the recurrence grows much faster (exponential), so master theorem does not apply directly.
- Always write the recurrence, then check if it matches the master theorem form.



Master Theorem (for T(N) = a·T(N/b) + f(N))
Identify a, b, and f(N):

a = number of subproblems
b = factor by which problem size is divided
f(N) = extra work done outside recursion
Calculate log_b(a):

This is the critical exponent: log base b of a.
Compare f(N) with N^{log_b(a)}:

Case 1: If f(N) = O(N^{log_b(a) - ε}) for some ε > 0 (f(N) grows slower),
then T(N) = Θ(N^{log_b(a)})
Case 2: If f(N) = Θ(N^{log_b(a)}) (f(N) grows at the same rate),
then T(N) = Θ(N^{log_b(a)} · log N)
Case 3: If f(N) = Ω(N^{log_b(a) + ε}) for some ε > 0 (f(N) grows faster),
and a·f(N/b) ≤ c·f(N) for some c < 1 and large N,
then T(N) = Θ(f(N))

Example: Merge Sort
Recurrence: T(N) = 2·T(N/2) + O(N)
a = 2, b = 2, f(N) = O(N)
log_2(2) = 1, so N^{log_2(2)} = N
f(N) = O(N), which is the same as N^{log_2(2)}
So, by Case 2: T(N) = Θ(N log N)
Summary Table (from image):
If f(N) is smaller: Θ(N^{log_b(a)})
If f(N) is same: Θ(N^{log_b(a)} · log N)
If f(N) is bigger: Θ(f(N))



